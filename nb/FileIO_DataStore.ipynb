{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data, files, IO\n",
    "\n",
    "author: Eric Charles, based on a similar NB by Sam Schmidt<br>\n",
    "Last successfully run: October 6, 2022<br>\n",
    "\n",
    "This notebook will demonstrate a variety of ways that users may interact with data using the descformats package<br>\n",
    "\n",
    "In additions to the descformats code, we have developed another companion package, `tables_io` [available here on Github](https://github.com/LSSTDESC/tables_io/). <br>\n",
    "\n",
    "`tables_io` aims to simplify IO for reading/writing to some of the most common file formats used within DESC: HDF5 files, parquet files, Astropy tables, and `qp` ensembles.  There are several examples of tables_io usage in the [nb directory](https://github.com/LSSTDESC/tables_io/tree/main/nb) of the `tables_io` repository, but we will demonstrate usage in several places in this notebook as well.  For furthe examples consult the tables_io nb examples.  \n",
    "\n",
    "In short, `tables_io` aims to simplify fileIO, and much of the io is automatically sorted out for you if your files have the appriorate extensions: that is, you can simply do a tables_io.read(\"file.fits\") to read in a fits file or tables_io.read(\"newfile.pq\") to read in a dataframe in parquet format.  \n",
    "\n",
    "Other concept used in the `descformats` used in a Jupyter Notebook are the DataStore and DataHandle.   These provide you with ways to access particular types of data.  We will demonstrate some useful features of the DataStore and the DataHandle below.\n",
    "\n",
    "Let's start out with some imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import descformats\n",
    "import tables_io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's use tables_io to read in some example data.  There are two example files that ship with descformats containing a small amount of cosmoDC2 data from healpix pixel `9816`, it is located in the `src/descformats/data/testdata` directory in the DESCFormats repository.  Let's read in one of those data files:\n",
    "\n",
    "(NOTE: for historical reasons, our examples files have data that is in hdf5 format where all of the data arrays are actually in a single hdf5 group named \"photometry\".  We will grab the data specifically from that hdf5 group by reading in the file and specifying [\"photometry\"] as the group in the cell below.  We'll call our dataset \"traindata_io\" to indicate that we've read it in via tables_io, and distinguish it from the data that we'll place in the DataStore in later steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = os.path.abspath(os.path.join(os.path.dirname(descformats.__file__), 'data'))\n",
    "trainFile = os.path.join(DATADIR, 'testdata/test_dc2_training_9816.hdf5')\n",
    "testFile = os.path.join(DATADIR, 'testdata/test_dc2_validation_9816.hdf5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tables_io reads this data in as an ordered dictionary of numpy arrays by default, though you can be converted to other data formats, such as a pandas dataframe as well. \n",
    "\n",
    "descformats wraps this functionality into a table handle object that you will be able to use as a reference to the data when building data analysis pipelines.\n",
    "\n",
    "Let's print out the keys in the ordered dict showing the available columns, then convert the data to a pandas dataframe and look at a few of the columns as a demo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handle = descformats.TableHandle('data', path=trainFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata_io = handle.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata_io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata_pq = tables_io.convert(traindata_io, tables_io.types.PD_DATAFRAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata_pq['photometry'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's set up the Data Store, so that our RAIL module will know where to fetch data.  We will set \"allow overwrite\" so that we can overwrite data files and not throw errors while in our jupyter notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from descformats.data import DATA_STORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS = DATA_STORE()\n",
    "DS.__class__.allow_overwrite = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to add our data to the DataStore, we can add previously read data, like our `traindata_pq`, or add data to the DataStore directly via the `DS.read_file` method, which we will do with our \"test data\".  We can add data with `DS.add_data` for the data already in memory, we want our data in a Numpy Ordered Dict, so we will specify the type as a TableHandle.  If, instead, we were storing a qp ensemble then we would set the handle as a `QPHandle`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add data that is already read in\n",
    "train_data = DS.add_data(\"train_data\", traindata_io, descformats.TableHandle )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read in data from file, we can use `DS.read_file`, once again we want a TableHandle, and we can feed it the `testFile` path defined in Cell #2 above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add test data directly to datastore from file:\n",
    "test_data = DS.read_file(\"test_data\", descformats.TableHandle, testFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's list the data abailable to us in the DataStore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the DataStore is just a dictionary of the files.  Each Handle object contains the actual data, which is accessible via the `.data` property for that file.  While not particularly designed for it, you can manipulate the data via these dictionaries, which is handy for on-the-fly exploration in notebooks.<br>\n",
    "For example, say we want to add an additional column to the train_data, say \"FakeID\" with a more simple identifier than the long ObjID that is contained the `id` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data().keys()\n",
    "numgals = len(train_data()['photometry']['id'])\n",
    "train_data()['photometry']['FakeID'] = np.arange(numgals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert our train_data to a pandas dataframe with tables_io, and our new \"FakeID\" column should now be present:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_table = tables_io.convertObj(train_data()['photometry'], tables_io.types.PD_DATAFRAME)\n",
    "train_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there it is, a new \"FakeID\" column is now added to the end of the dataset, success!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using DataHandle objects to read data\n",
    "\n",
    "There are a variety of ways that you can use a DataHandle, \n",
    "\n",
    "1. reading all the data at once with the read() function\n",
    "2. opening the file, but not reading the data\n",
    "3. iterating over the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's start by reading all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_handle = descformats.TableHandle('data', path=trainFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_handle.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the data are stored in the DataHandle object, so you can do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_handle.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note also that the file has been closed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_handle.fileObj is None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Opening the DataHandle\n",
    "\n",
    "First, let's reset the data in the data handle, then we will use the open() method to open the file object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_handle.data = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_handle.open()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_handle.fileObj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_handle.data is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_handle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_handle.fileObj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterating over the data\n",
    "\n",
    "Here we use the DataHandle to create an iterator over the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = new_handle.iterator(groupname='photometry', chunk_size=1000)\n",
    "print(x)\n",
    "for xx in x:\n",
    "    print(xx[0], xx[1], xx[2]['id'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
